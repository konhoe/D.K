# scripts/smoke_cpu.py
from __future__ import annotations
import os
import torch
from torch.utils.data import DataLoader

from transformers import set_seed

from src import (
    DeepfakeModel,
    prepare_deepfake_dataset,
    attach_media_transforms,
    get_collate_fn_stage1,
    get_collate_fn_stage2
)

def main():
    set_seed(42)

    print("âœ… CPU ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # ------------------------------------------------
    # 1) ë°ì´í„° ë¡œë“œ (ë„ˆê°€ ë°©ê¸ˆ ì“°ë˜ metadata ê²½ë¡œ ê·¸ëŒ€ë¡œ)
    #    â†’ ì¼ë‹¨ 200ê°œë§Œ ì¨ì„œ ë¹ ë¥´ê²Œ í™•ì¸
    # ------------------------------------------------
    train_data, test_data, label2id, id2label, class_labels, image_key, video_key = (
        prepare_deepfake_dataset(
            data_path=None,
            data_files="/Users/junyoung/Desktop/baseline/metadata_1000.tsv",  # ë„¤ tsv ê²½ë¡œ
            delimiter="\t",
            split="train",
            test_size=0.2,
            seed=42,
        )
    )

    # ë„ˆë¬´ ë§ìœ¼ë©´ ëŠë¦¬ë‹ˆê¹Œ ì•ì—ì„œ ëª‡ ê°œë§Œ ì˜ë¼ì„œ í™•ì¸
    train_small = train_data.select(range(min(32, len(train_data))))

    print(f"ğŸ”¹ train_small size = {len(train_small)}")
    print(f"ğŸ”¹ image_key = {image_key}, video_key = {video_key}")

    # ------------------------------------------------
    # 2) ì „ì²˜ë¦¬ í›… ë¶€ì°© (ì´ë¯¸ì§€/ë¹„ë””ì˜¤ â†’ pixel_values í…ì„œ)
    #    ì§€ê¸ˆì€ num_frames=1 ë¡œ ì´ë¯¸ì§€ ìŠ¤í…Œì´ì§€ë§Œ í™•ì¸
    # ------------------------------------------------
    attach_media_transforms(
        train_small,
        test_data,  # ì•ˆ ì¨ë„ ë˜ì§€ë§Œ í˜•ì‹ìƒ ë„˜ê²¨ì¤Œ
        image_key=image_key,
        video_key=video_key,
        clip_model_name="openai/clip-vit-large-patch14",
        do_face_crop=False,
        rotation_deg=15,
        num_frames=12,  # CPU í…ŒìŠ¤íŠ¸ë‹ˆê¹Œ 1ì¥ë§Œ
    )

    # ------------------------------------------------
    # 3) DataLoader (Stage-1ìš© collate_fn: (B,3,H,W) ë‚˜ì˜¤ëŠ” ë²„ì „)
    # ------------------------------------------------
    loader = DataLoader(
        train_small,
        batch_size=4,
        shuffle=True,
        num_workers=0,              
        collate_fn=get_collate_fn_stage2(),  
    )

    # ------------------------------------------------
    # 4) ëª¨ë¸ ìƒì„± (CPU, fp32, frame=1 ê¸°ì¤€ cfg)
    # ------------------------------------------------
    device = torch.device("cpu")
    num_classes = len(class_labels.names)

    model = DeepfakeModel(
        clip_model_name="openai/clip-vit-large-patch14",
        dtype="fp32",              # CPUì—ì„œëŠ” fp32ê°€ ì œì¼ ì•ˆì „
        freeze_backbone=True,      # ì¼ë‹¨ backbone ë™ê²°
        unfreeze_last_n_blocks=0,
        num_classes=num_classes,
        d2st_num_frames=12,         # ì§€ê¸ˆì€ ì´ë¯¸ì§€ í•œ ì¥ë§Œ â†’ T=1
        d2st_scale=0.25,
        hidden_mult=2,
        temporal_pool="mean",
        id2label=id2label,
        label2id=label2id,
    ).to(device)

    model.eval()

    # ------------------------------------------------
    # 5) í•œë‘ ë°°ì¹˜ë§Œ forward ëŒë ¤ë³´ê¸°
    # ------------------------------------------------
    with torch.no_grad():
        for i, batch in enumerate(loader):
            x = batch["pixel_values"].to(device)  # (B,3,H,W)
            y = batch["labels"].to(device)        # (B,)

            print(f"\n[batch {i}]")
            print(f"  pixel_values.shape = {x.shape}")
            print(f"  labels.shape       = {y.shape}")
            print(f"  labels[:8]         = {y[:8]}")

            # âœ¨ ì—¬ê¸°ì„œ ì‹¤ì œ ëª¨ë¸ forward
            logits = model(pixel_values=x)        # mode=None â†’ dim=4ë¼ image branch
            probs = torch.softmax(logits, dim=-1)

            print(f"  logits.shape       = {logits.shape}")  # (B, num_classes)
            print(f"  probs[0]           = {probs[0]}")

            # í•œë‘ ë°°ì¹˜ë§Œ ë³´ë©´ ë˜ë‹ˆê¹Œ
            if i >= 2:
                break

    print("\nâœ… CPU ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
