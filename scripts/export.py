import os
import json
import torch

from transformers import CLIPImageProcessor, CLIPModel
from safetensors.torch import save_file

from src import ClipBackbone, DeepfakeModel   # ë„¤ íŒ¨í‚¤ì§€ êµ¬ì¡° ê¸°ì¤€

# ==========================
# ì„¤ì •
# ==========================
CHECKPOINT_PATH = "./outputs_stage1_1/checkpoint-37680/pytorch_model.bin"
EXPORT_DIR = "./model/clip_base"
os.makedirs(EXPORT_DIR, exist_ok=True)

clip_name = "openai/clip-vit-large-patch14"

# label ë§¤í•‘ (í•™ìŠµí•  ë•Œ ì“°ë˜ ê±°ë‘ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ë©´ ë¨)
id2label = {0: "real", 1: "fake"}
label2id = {"real": 0, "fake": 1}

print("=" * 60)
print("ğŸš€ Starting export for OFFLINE submission (DeepfakeModel + D2ST)")
print("=" * 60)

# ================================================================
# (1) CLIP backboneì„ ë¡œì»¬ì— ì €ì¥ (í—ˆê¹…í˜ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
# ================================================================
print("\n[1/7] Saving CLIP backbone locally...")
clip_model = CLIPModel.from_pretrained(clip_name)
backbone_dir = os.path.join(EXPORT_DIR, "clip_backbone")
clip_model.save_pretrained(backbone_dir)
print(f"  âœ… CLIP saved to: {backbone_dir}")

# ================================================================
# (2) DeepfakeModel ì¬êµ¬ì„± (ë¡œì»¬ CLIP ì‚¬ìš©)
# ================================================================
print("\n[2/7] Building DeepfakeModel with local ClipBackbone...")


model = DeepfakeModel(
    clip_model_name=backbone_dir, 
    dtype="fp32",
    freeze_backbone=True,
    unfreeze_last_n_blocks=0,
    num_classes=2,
    d2st_num_frames=1,      
    d2st_scale=0.25,        
    hidden_mult=2,
    temporal_pool="mean",
    id2label=id2label,
    label2id=label2id,
)
print("  âœ… Model structure created")

# ================================================================
# (3) í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
# ================================================================
print(f"\n[3/7] Loading checkpoint from: {CHECKPOINT_PATH}")
if not os.path.exists(CHECKPOINT_PATH):
    print(f"âŒ ERROR: Checkpoint not found at {CHECKPOINT_PATH}")
    raise SystemExit(1)

state = torch.load(CHECKPOINT_PATH, map_location="cpu")
print(f"  - Checkpoint keys: {len(state.keys())}")

# strict=Falseë¡œ ë¡œë“œ (position_ids ë“± ë²„ì „ ì°¨ì´ ëŒ€ì‘)
incompatible = model.load_state_dict(state, strict=False)
print(f"  - Missing keys: {len(incompatible.missing_keys)}")
if incompatible.missing_keys:
    position_missing = [k for k in incompatible.missing_keys if "position" in k.lower()]
    critical_missing = [k for k in incompatible.missing_keys if "position" not in k.lower()]
    print(f"    â€¢ Position-related (OK): {len(position_missing)}")
    if critical_missing:
        print(f"    â€¢ âš ï¸ Critical missing: {critical_missing}")

print(f"  - Unexpected keys: {len(incompatible.unexpected_keys)}")
if incompatible.unexpected_keys:
    print(f"    {incompatible.unexpected_keys[:5]}...")

print("  âœ… Checkpoint loaded")

# ================================================================
# (4) ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì €ì¥
# ================================================================
print("\n[4/7] Saving image processor...")
processor = CLIPImageProcessor.from_pretrained(clip_name)
processor.save_pretrained(EXPORT_DIR)
print(f"  âœ… Processor saved to: {EXPORT_DIR}")

# ================================================================
# (5) Adapter + Headë§Œ ì¶”ì¶œí•´ì„œ ì €ì¥ (ë°±ë³¸ ì œì™¸)
# ================================================================
print("\n[5/7] Extracting adapter + head weights...")

# backbone.* ì€ CLIP / ViT ìª½, ë‚˜ë¨¸ì§€ê°€ adapter + classifier
adapter_state = {
    k: v for k, v in model.state_dict().items()
    if not k.startswith("backbone.")
}
print(f"  - Adapter+Head parameters: {len(adapter_state)}")
print(f"  - Sample keys: {list(adapter_state.keys())[:5]}")

adapter_path = os.path.join(EXPORT_DIR, "adapter_head.safetensors")
save_file(adapter_state, adapter_path)
adapter_size = os.path.getsize(adapter_path) / (1024**2)
print(f"  âœ… Adapter saved: {adapter_path} ({adapter_size:.2f} MB)")

# ================================================================
# (6) ì œì¶œìš© ì „ì²´ ëª¨ë¸ ì €ì¥ (í•„í„°ë§ ê¸ˆì§€ + position_ids ë³´ê°•)
# ================================================================
print("\n[6/7] Saving FULL model for submission...")


def ensure_position_ids(sd: dict, mdl: DeepfakeModel) -> dict:
    """ì œì¶œ(strict=True) í™˜ê²½ í˜¸í™˜ì„ ìœ„í•´ position_ids 3ì¢…ì„ ë³´ê°•."""
    # í…ìŠ¤íŠ¸ ê¸¸ì´ (CLIP text ë³´í†µ 77)
    try:
        txt_conf = mdl.backbone.clip.text_model.config
        txt_len = int(getattr(txt_conf, "max_position_embeddings", 77))
    except Exception:
        txt_len = 77
    txt_pos = torch.arange(0, txt_len, dtype=torch.long).unsqueeze(0)

    # ë¹„ì „ ê¸¸ì´ (ViT-L/14: 224/14=16 â†’ 16*16+1=257)
    try:
        vconf = mdl.backbone.clip.vision_model.config
        v_len = (vconf.image_size // vconf.patch_size) ** 2 + 1
    except Exception:
        v_len = 257
    vis_pos = torch.arange(0, v_len, dtype=torch.long).unsqueeze(0)

    required = {
        "backbone.clip.text_model.embeddings.position_ids": txt_pos,
        "backbone.clip.vision_model.embeddings.position_ids": vis_pos,
        "backbone.vision.embeddings.position_ids": vis_pos,  # alias
    }
    for k, v in required.items():
        if k not in sd:
            sd[k] = v
    return sd


# â˜… ì „ì²´ state_dict (backbone + adapter + head)
full_state_all = model.state_dict()
full_state_all = ensure_position_ids(full_state_all, model)

submit_model_path = os.path.join(EXPORT_DIR, "model.bin")
torch.save(full_state_all, submit_model_path)
full_size_all = os.path.getsize(submit_model_path) / (1024**2)
print(f"  âœ… Submission model saved: {submit_model_path} ({full_size_all:.2f} MB)")

# (ì˜µì…˜) text ê´€ë ¨ í‚¤ ì œì™¸í•œ ë””ë²„ê·¸ìš© ë²„ì „
debug_filtered = {
    k: v for k, v in full_state_all.items()
    if ("text_model" not in k and "clip.text" not in k)
}
debug_path = os.path.join(EXPORT_DIR, "pytorch_model.debug_filtered.bin")
torch.save(debug_filtered, debug_path)
debug_size = os.path.getsize(debug_path) / (1024**2)
print(f"  ğŸ§ª Debug filtered model saved: {debug_path} ({debug_size:.2f} MB)")

# í•„ìˆ˜ position_ids í‚¤ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
for k in [
    "backbone.clip.text_model.embeddings.position_ids",
    "backbone.clip.vision_model.embeddings.position_ids",
    "backbone.vision.embeddings.position_ids",
]:
    assert k in full_state_all, f"âŒ Missing required key for submission: {k}"
print("  ğŸ” Required position_ids keys verified.")

# ================================================================
# (7) ì»¤ìŠ¤í…€ ì„¤ì • ì €ì¥
# ================================================================
print("\n[7/7] Saving custom config...")

cfg = {
    "model_type": "deepfake_d2st",   # ì˜ˆì „ unified_adapter ëŒ€ì‹ 
    "clip_model_name": "clip_backbone",  # ë¡œì»¬ í´ë” ì´ë¦„ (EXPORT_DIR/clip_backbone)
    "num_frames": 1,                
    "adapter_type": "d2st_v1",       # ê·¸ëƒ¥ ë©”íƒ€ ì •ë³´
    "temporal_pool": "mean",
    "head_hidden": 2 * model.backbone.embed_dim,  # hidden_mult=2 ê¸°ì¤€
    "num_classes": 2,
    "id2label": id2label,
    "label2id": label2id,
}
config_path = os.path.join(EXPORT_DIR, "custom_config.json")
with open(config_path, "w") as f:
    json.dump(cfg, f, indent=2, ensure_ascii=False)
print(f"  âœ… Config saved: {config_path}")

print("\nâœ… Export DONE.")
