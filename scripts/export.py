import os, json, torch
from transformers import CLIPImageProcessor, CLIPModel
from safetensors.torch import save_file
from src import ClipBackbone, UnifiedAdapterModel

CHECKPOINT_PATH = "./outputs/checkpoint-35690/pytorch_model.bin"
EXPORT_DIR = "./model/clip_base"
os.makedirs(EXPORT_DIR, exist_ok=True)

clip_name = "openai/clip-vit-large-patch14"

print("="*60)
print("ğŸš€ Starting export for OFFLINE submission")
print("="*60)

# ================================================================
# (1) CLIP backboneì„ ë¡œì»¬ì— ì €ì¥ (í—ˆê¹…í˜ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
# ================================================================
print("\n[1/7] Saving CLIP backbone locally...")
clip_model = CLIPModel.from_pretrained(clip_name)
backbone_dir = os.path.join(EXPORT_DIR, "clip_backbone")
clip_model.save_pretrained(backbone_dir)
print(f"  âœ… CLIP saved to: {backbone_dir}")

# ================================================================
# (2) ìˆ˜ì •ëœ ClipBackboneìœ¼ë¡œ ëª¨ë¸ ì¬êµ¬ì„±
# ================================================================
print("\n[2/7] Building model with ClipBackbone...")
backbone = ClipBackbone(
    model_name=backbone_dir,  # ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©
    dtype="fp32",
    freeze_backbone=True
)
model = UnifiedAdapterModel(
    backbone=backbone,
    num_frames=12,
    adapter_type="tconv",
    temporal_pool="mean",
    head_hidden=1024,
    num_classes=2,
    id2label={0: "real", 1: "fake"},
    label2id={"real": 0, "fake": 1}
)
print(f"  âœ… Model structure created")

# ================================================================
# (3) í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
# ================================================================
print(f"\n[3/7] Loading checkpoint from: {CHECKPOINT_PATH}")
if not os.path.exists(CHECKPOINT_PATH):
    print(f"âŒ ERROR: Checkpoint not found at {CHECKPOINT_PATH}")
    raise SystemExit(1)

state = torch.load(CHECKPOINT_PATH, map_location="cpu")
print(f"  - Checkpoint keys: {len(state.keys())}")

# strict=Falseë¡œ ë¡œë“œ (position_ids ë“± ë²„ì „ ì°¨ì´ ëŒ€ì‘)
incompatible = model.load_state_dict(state, strict=False)
print(f"  - Missing keys: {len(incompatible.missing_keys)}")
if incompatible.missing_keys:
    position_missing = [k for k in incompatible.missing_keys if 'position' in k.lower()]
    critical_missing = [k for k in incompatible.missing_keys if 'position' not in k.lower()]
    print(f"    â€¢ Position-related (OK): {len(position_missing)}")
    if critical_missing:
        print(f"    â€¢ âš ï¸ Critical missing: {critical_missing}")

print(f"  - Unexpected keys: {len(incompatible.unexpected_keys)}")
if incompatible.unexpected_keys:
    print(f"    {incompatible.unexpected_keys[:3]}...")

print(f"  âœ… Checkpoint loaded")

# ================================================================
# (4) ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì €ì¥
#    â€» ëª¨ë¸ ì €ì¥ê³¼ ë³„ê°œë¡œ ì „ì²˜ë¦¬ ì„¤ì •ì„ ë¡œì»¬ì— ê³ ì •í•´ë‘ (ì˜¤í”„ë¼ì¸ ì¶”ë¡ )
# ================================================================
print(f"\n[4/7] Saving image processor...")
# (ì£¼ì˜) processorëŠ” model.save_pretrainedì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ model ë””ë ‰í† ë¦¬ì—ì„œ ì½ì§€ ë§ê³  ì›ë³¸ì—ì„œ ë°›ì•„ ì €ì¥
processor = CLIPImageProcessor.from_pretrained(clip_name)
processor.save_pretrained(EXPORT_DIR)
print(f"  âœ… Processor saved to: {EXPORT_DIR}")

# ================================================================
# (5) Adapter+Headë§Œ ì¶”ì¶œí•´ì„œ ì €ì¥ (ê¶Œì¥: ëª¨ë“ˆí˜• ë°°í¬/ë””ë²„ê¹…ìš©)
# ================================================================
print("\n[5/7] Extracting adapter+head weights...")
adapter_state = {k: v for k, v in model.state_dict().items() if not k.startswith("backbone.")}
print(f"  - Adapter+Head parameters: {len(adapter_state)}")
print(f"  - Sample keys: {list(adapter_state.keys())[:5]}")

adapter_path = os.path.join(EXPORT_DIR, "adapter_head.safetensors")
save_file(adapter_state, adapter_path)
adapter_size = os.path.getsize(adapter_path) / (1024**2)
print(f"  âœ… Adapter saved: {adapter_path} ({adapter_size:.2f} MB)")

# ================================================================
# (6) ì œì¶œìš© ì „ì²´ ëª¨ë¸ ì €ì¥ (í•„í„°ë§ ê¸ˆì§€ + position_ids ë³´ê°•)
# ================================================================
print("\n[6/7] Saving FULL model for submission...")

def ensure_position_ids(sd: dict, mdl: UnifiedAdapterModel) -> dict:
    """ì œì¶œ(strict=True) í™˜ê²½ í˜¸í™˜ì„ ìœ„í•´ position_ids 3ì¢…ì„ ë³´ê°•."""
    import torch

    # í…ìŠ¤íŠ¸ ê¸¸ì´ (CLIP text ë³´í†µ 77)
    try:
        txt_conf = mdl.backbone.clip.text_model.config
        txt_len = int(getattr(txt_conf, "max_position_embeddings", 77))
    except Exception:
        txt_len = 77
    txt_pos = torch.arange(0, txt_len, dtype=torch.long).unsqueeze(0)

    # ë¹„ì „ ê¸¸ì´ (ViT-L/14: 224/14=16 â†’ 16*16+1=257)
    try:
        vconf = mdl.backbone.clip.vision_model.config
        v_len = (vconf.image_size // vconf.patch_size) ** 2 + 1
    except Exception:
        v_len = 257
    vis_pos = torch.arange(0, v_len, dtype=torch.long).unsqueeze(0)

    # ì œì¶œ ìª½ì´ ê¸°ëŒ€í•˜ëŠ” 3ê°œ í‚¤ ëª¨ë‘ ë³´ì¥
    required = {
        "backbone.clip.text_model.embeddings.position_ids": txt_pos,
        "backbone.clip.vision_model.embeddings.position_ids": vis_pos,
        "backbone.vision.embeddings.position_ids": vis_pos,  # ì¼ë¶€ êµ¬í˜„ì—ì„œ ì´ aliasë¥¼ ìš”êµ¬
    }
    for k, v in required.items():
        if k not in sd:
            sd[k] = v
    return sd

full_state_all = model.state_dict()             # â˜… í•„í„°ë§ ì—†ì´ ì „ì²´ ì €ì¥
full_state_all = ensure_position_ids(full_state_all, model)

submit_model_path = os.path.join(EXPORT_DIR, "model.bin")  # ì œì¶œ ê·œê²©ì— í”í•œ ì´ë¦„
torch.save(full_state_all, submit_model_path)
full_size_all = os.path.getsize(submit_model_path) / (1024**2)
print(f"  âœ… Submission model saved: {submit_model_path} ({full_size_all:.2f} MB)")

# (ì˜µì…˜) ë””ë²„ê¹…/ë¹„êµìš©: text ê´€ë ¨ í‚¤ ì œì™¸í•œ í•„í„° ë²„ì „ë„ í•¨ê»˜ ë³´ê´€ (ì œì¶œìš© ì•„ë‹˜)
debug_filtered = {k: v for k, v in full_state_all.items() if ('text_model' not in k and 'clip.text' not in k)}
debug_path = os.path.join(EXPORT_DIR, "pytorch_model.debug_filtered.bin")
torch.save(debug_filtered, debug_path)
debug_size = os.path.getsize(debug_path) / (1024**2)
print(f"  ğŸ§ª Debug filtered model saved: {debug_path} ({debug_size:.2f} MB)")

# ê°„ì´ ê²€ì¦: í•„ìš”í•œ í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
for k in [
    "backbone.clip.text_model.embeddings.position_ids",
    "backbone.clip.vision_model.embeddings.position_ids",
    "backbone.vision.embeddings.position_ids",
]:
    assert k in full_state_all, f"âŒ Missing required key for submission: {k}"
print("  ğŸ” Required position_ids keys verified.")

# ================================================================
# (7) ì»¤ìŠ¤í…€ ì„¤ì • ì €ì¥
# ================================================================
print("\n[7/7] Saving config...")
cfg = {
    "model_type": "unified_adapter",
    "clip_model_name": "clip_backbone",  # ë¡œì»¬ ë””ë ‰í† ë¦¬ ì°¸ì¡°
    "num_frames": 12,
    "adapter_type": "tconv",
    "temporal_pool": "mean",
    "head_hidden": 1024,
    "num_classes": 2,
    "id2label": {0: "real", 1: "fake"},
    "label2id": {"real": 0, "fake": 1}
}
config_path = os.path.join(EXPORT_DIR, "custom_config.json")
with open(config_path, "w") as f:
    json.dump(cfg, f, indent=2)
print(f"  âœ… Config saved: {config_path}")

# ================================================================
# ìµœì¢… íŒŒì¼ ë¦¬ìŠ¤íŠ¸
# ================================================================